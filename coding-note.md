# Write down the things you think important

## Tokenization

`regexp_tokenize(lyrics.lower(), "[a-z]['a-z*]")`  
This is not working, the lyrics is cutted to strange parts like

> {'ya': 0.8261843799448573, 'lo': 0.269408102754446, 'ok': 0.5338681366340317, 'ra': 0.28334506027850553, 'ab': 0.7743083311600188, 'ou': 0.18730920202491874, 'th': 0.03654653490780818, 'cr': 0.39400328056279504, 'ap': 0.8063638509112071, 'ro': 0.23868930561955023, 'ut': 0.9084208250971048, 'yo': 0.09002065489301278, 'pr': 0.578895488733821, 'ob': 1.2211302539259055, 'ly': 0.48835691064445885, 'wa': 0.23585231654779376, 'nt': 0.3613672027083882, 'ta': 0.4270561338456427, 'ke': 0.19975571877255494, 'ca': 0.18133457763307045, 'us': 0.31530593750517655, 'ur': 0.4703247764852994, 'ju': 0.4186352353972383, 'st': 0.08161918432793328, 'dy': 2.499343486964679, 'ei': 1.7968634544227373, 'ng': 0.2017281344644223, 'kn': 0.4256769304922307, 'ow': 0.2531499425808477, 'wh': 0.15505668611096782, 

instead, use this
'''python
 punctuation = re.compile(r'[^\w\s\']')
    
lyrics_split = re.sub(punctuation, ' ', lyrics.lower()).split()
'''
