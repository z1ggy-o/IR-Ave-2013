{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Most Representive songs of the singer\n",
    "\n",
    "## Idea\n",
    "\n",
    "1. Have lyrics of each singer, use that to build a VSM\n",
    "2. Sort lyric vectors, find the most important terms of each song (like top 30 terms)\n",
    "3. Get all the important terms of each song together, sort them by df, then choose the top [100] terms as the represent terms of this singer.\n",
    "4. For each lyric vector, sum the representive terms' weight together, treat it as this song's score\n",
    "5. Find the top 20 songs through the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class List:\n",
    "    class Node:\n",
    "        def __init__(self, doc,dtf): #node는 doc_number,dtf link로 구성\n",
    "            self.doc = doc\n",
    "            self.dtf=dtf\n",
    "            self.next = None\n",
    "\n",
    "    def __init__(self,term): #head는 term,freq,link로 구성\n",
    "        self.head = None\n",
    "        self.term=term\n",
    "        self.freq = 0\n",
    "\n",
    "    def freq(self):\n",
    "        return self.freq\n",
    "\n",
    "    def term(self):\n",
    "        return self.term\n",
    "\n",
    "    def add(self,doc,dtf):\n",
    "        p=self.head\n",
    "        if p==None:\n",
    "            self.head=self.Node(doc,dtf)\n",
    "        else:\n",
    "            while (p.next != None):\n",
    "                p=p.next\n",
    "            p.next = self.Node(doc,dtf)\n",
    "        self.freq += 1\n",
    "\n",
    "    def print_list(self): # doc_number를 출력하기 위한 함수\n",
    "        p=self.head\n",
    "        res=[]\n",
    "        while p is not None:\n",
    "            res.append(p.doc)\n",
    "            p=p.next\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _singer_song_dic(data):\n",
    "    '''Build singer-song dictionay\n",
    "    \n",
    "    Dictionary format:\n",
    "    {singer_name: {song_name: lyrics}}\n",
    "    '''\n",
    "    \n",
    "    singer_songs = {}\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        singer = data.iloc[i]['artist']\n",
    "        song = data.iloc[i]['song']\n",
    "        lyric = data.iloc[i]['lyrics']\n",
    "        \n",
    "        \n",
    "        if singer not in singer_songs:\n",
    "            temp = {song: lyric}\n",
    "            singer_songs[singer] = temp\n",
    "        else:\n",
    "            singer_songs[singer][song] = lyric\n",
    "            \n",
    "    return singer_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is tested\n",
    "def _update_inverted_index(name, lyrics, inverted_index):\n",
    "    '''Create inverted index, count doc vector length\n",
    "\n",
    "    Read contents form file, remove punctuation and stopwords to get terms.\n",
    "    Count tf of this doc, then update inverted index.\n",
    "    \n",
    "    inverted_index\n",
    "    '''\n",
    "    \n",
    "    indices = {}\n",
    "    punctuation = re.compile(r'[^\\w\\s\\']')\n",
    "    \n",
    "    ###\n",
    "    # Count term frequency\n",
    "    ###\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lyrics_split = re.sub(punctuation, ' ', lyrics.lower()).split()\n",
    "    \n",
    "    for term in lyrics_split:\n",
    "        if term in stop_words:\n",
    "            continue\n",
    "        elif term in indices:\n",
    "            indices[term] += 1\n",
    "        else:\n",
    "            indices[term] = 1\n",
    "    \n",
    "    ###\n",
    "    # Update inverted_index\n",
    "    ###\n",
    "    for term, frequency in indices.items():\n",
    "        if term in inverted_index:\n",
    "            posting = inverted_index[term]\n",
    "            posting.append((name, frequency))\n",
    "            inverted_index[term] = posting\n",
    "        else:\n",
    "            inverted_index[term] = [(name, frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_weight(tf, df, n_songs):\n",
    "    \n",
    "    idf = math.log(n_songs / df)\n",
    "\n",
    "    tf_normalized = 1 + math.log(tf)\n",
    "\n",
    "    weight = tf_normalized * idf\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_song_vector(inverted_index, n_songs):\n",
    "    '''Build term-weight vector for songs\n",
    "    \n",
    "    Compute the tf-idf weight, {term: weight}\n",
    "    \n",
    "    return:\n",
    "        dic: a dictionary which format is {song: {term: weight}}\n",
    "    '''\n",
    "    \n",
    "    song_vectors = {}\n",
    "    \n",
    "    for term, posting in inverted_index.items():\n",
    "        df = len(posting)\n",
    "        for pair in posting:\n",
    "            song, tf = pair\n",
    "            weight = _compute_weight(tf, df, n_songs)\n",
    "            if song not in song_vectors:  # Create song vec\n",
    "                song_vectors[song] = {term: weight}\n",
    "            else:\n",
    "                song_vectors[song][term] = weight  # Add new term into vec\n",
    "                \n",
    "    return song_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_top_songs(inverted_index, terms):\n",
    "    '''Count rep terms' frequency, use that to pick rep songs\n",
    "    \n",
    "    Args:\n",
    "        inverted_index: inverted index of this singer's songs\n",
    "            format => {term: [(song, tf),]}\n",
    "        terms: this singer's representive terms\n",
    "    Return:\n",
    "        top_songs(list): a list that contains the name of songs, order by score\n",
    "    '''\n",
    "    \n",
    "    song_scores = {}\n",
    "    for term in terms:\n",
    "        posting = inverted_index[term]\n",
    "        for song, _ in posting:\n",
    "            if song in song_scores:\n",
    "                song_scores[song] += 1\n",
    "            else:\n",
    "                song_scores[song] = 1\n",
    "                \n",
    "    top_songs = sorted(song_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    top_songs = [name for name, _ in top_songs]\n",
    "    return top_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_rep_songs(songs, num_of_songs, num_of_terms):\n",
    "    '''Find the most representive songs of this singer\n",
    "    \n",
    "    Choose songs by compare the scores which is sum the weight of each important term in that song.\n",
    "    \n",
    "    Args:\n",
    "        songs(dic): a dictionary {song: lyrics}\n",
    "        num_of_songs: number of representive songs you want to choose\n",
    "        num_of_terms: number of important words we want to score the songs\n",
    "        \n",
    "    Return:\n",
    "        rep_songs(dic): a dictionary {song: lyrics}, size will less or equal to num_of_songs\n",
    "    '''\n",
    "    \n",
    "    inverted_index = {}\n",
    "    \n",
    "    n_songs = len(songs)\n",
    "    if n_songs <= num_of_songs:  # Do not need to choose if not has enough songs\n",
    "        return songs\n",
    "    \n",
    "    # Build inverted index\n",
    "    for name, lyrics in songs.items():\n",
    "        _update_inverted_index(name, lyrics, inverted_index)\n",
    "        \n",
    "    # build song vector\n",
    "    song_vectors = _build_song_vector(inverted_index, n_songs)\n",
    "    \n",
    "    # sort by weight, get top words (put them into a set)\n",
    "    selected_terms = set()\n",
    "    \n",
    "    for song, vector in song_vectors.items():\n",
    "        sorted_v = sorted(vector.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        select_range = sorted_v[:num_of_terms + 1]\n",
    "        for term, _ in select_range:\n",
    "            selected_terms.add(term)\n",
    "    \n",
    "    # get score of each song\n",
    "    song_sorted = _get_top_songs(inverted_index, selected_terms)\n",
    "    selected_songs = song_sorted[:num_of_songs + 1]\n",
    "    top_songs = {}\n",
    "    for name, lyrics in songs.items():\n",
    "        if name in selected_songs:\n",
    "            top_songs[name] = lyrics\n",
    "    \n",
    "    return top_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_songs(num_of_songs, num_of_terms, file_path):\n",
    "    '''Get each singer's representive songs\n",
    "    \n",
    "    Args: \n",
    "        num_of_songs: number of songs you want to get from each singer\n",
    "        file_path: path of input data file\n",
    "    Return:\n",
    "        dic: A dictionary which format is {singer, {song: lyrics}}\n",
    "    '''\n",
    "    \n",
    "    # Read data from file\n",
    "    data = pd.read_csv(file_path)\n",
    "    singer_songs = _singer_song_dic(data)\n",
    "    \n",
    "    representive_songs = {}  # return dic\n",
    "    for singer, songs in singer_songs.items():\n",
    "        rep = _find_rep_songs(songs, num_of_songs, num_of_terms)\n",
    "        representive_songs[singer] = rep\n",
    "    \n",
    "    with open('top_songs.pickle', 'wb') as handle:\n",
    "        pickle.dump(representive_songs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        \n",
    "    return representive_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTing part\n",
    "\n",
    "files = 'mylyrics00.csv'\n",
    "dic=get_rep_songs(20, 50, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bet-shady-2-0-cypher', 'love-game', 'rap-god', 'zane-lowe-bbc-radio-interview-part-1', 'bad-guy', 'zane-lowe-bbc-radio-interview-part-2', 'rap-god-french-version', 'evil-twin', 'westwood-freestyle-2010', 'quitter', 'just-rhymin-wit-proof', 'campaign-speech', '2-0-boys', 'loud-noises', 'shady-2-0-cypher', 'detroit-vs-everybody', 'shady-xv-cypher', 'calm-down', 'shadyxv', 'vegas', 'detroit-vs-everybody-remix'])\n"
     ]
    }
   ],
   "source": [
    "doc=[]\n",
    "singer_list=list(dic.keys())\n",
    "song_list=list(dic.values())\n",
    "print((song_list[0].keys()))\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "for i in range(len(song_list)):\n",
    "    for key in song_list[i].keys():\n",
    "        temp=song_list[i][key]\n",
    "        temp = temp.lower()\n",
    "        list_temp = regexp_tokenize(temp, \"[a-z]['a-z]*\")\n",
    "\n",
    "        result = []\n",
    "        for w in list_temp:\n",
    "            if w not in stop_word:\n",
    "                result.append(w)\n",
    "        \n",
    "        doc.append([singer_list[i],key,result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listing(index): #LINKED LIST head:term,doc_freq // node: doc_num, freq in doc_num\n",
    "    list_set=[]\n",
    "\n",
    "    tmp_t=index[0][0]\n",
    "    tmp_d=index[0][1]\n",
    "    cnt=1\n",
    "    for i in range(1,len(index)):\n",
    "        tmp_term=index[i][0]\n",
    "        tmp_doc=index[i][1]\n",
    "\n",
    "        if(tmp_term==tmp_t and tmp_doc==tmp_d):\n",
    "           cnt+=1\n",
    "        else:\n",
    "            list_set.append([tmp_t,tmp_d,cnt])\n",
    "            cnt=1\n",
    "        tmp_t=tmp_term\n",
    "        tmp_d=tmp_doc\n",
    "    list_set.append([tmp_t, tmp_d, cnt])\n",
    "\n",
    "    list_all=[]\n",
    "    term=list_set[0][0]\n",
    "    list_t=List(term)\n",
    "    list_t.add(list_set[0][1],list_set[0][2])\n",
    "    for i in range(1,len(list_set)):\n",
    "        if(term!=list_set[i][0]):\n",
    "            list_all.append(list_t)\n",
    "            list_t=List(list_set[i][0])\n",
    "        list_t.add(list_set[i][1],list_set[i][2])\n",
    "        term=list_set[i][0]\n",
    "    list_all.append(list_t)\n",
    "\n",
    "    return list_all\n",
    "\n",
    "def indexing():  #vsm\n",
    "    vsm_word1 = []\n",
    "    index_doc=[]\n",
    "    indexed_list1=[]\n",
    "    for i in range(len(doc)):\n",
    "        for j in range(len(doc[i][2])):\n",
    "            index_doc.append([doc[i][2][j],i])\n",
    "\n",
    "    index_doc.sort(key=itemgetter(0))\n",
    "\n",
    "    indexed_list1=listing(index_doc)\n",
    "\n",
    "    for i in range(len(indexed_list1)):  # 각 단어별 weight 계산 단어 1개\n",
    "        vsm_word1.append([0 for j in range(len(doc) + 1)])\n",
    "        vsm_word1[i][0] = indexed_list1[i].term\n",
    "        p = indexed_list1[i].head\n",
    "        while (p != None):\n",
    "            w = (1 + math.log2(p.dtf)) * math.log2(len(doc) / indexed_list1[i].freq)\n",
    "            #w=p.dtf\n",
    "            vsm_word1[i][p.doc + 1] = float(w)\n",
    "            p = p.next\n",
    "    return vsm_word1\n",
    "\n",
    "vsm=indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "def lsa_model(str,vsm_vector):\n",
    "\n",
    "    str=str.lower()\n",
    "    temp_query=regexp_tokenize(str, \"[a-z]['a-z]*\")\n",
    "    \n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    list_query = []\n",
    "    for w in temp_query:\n",
    "        if w not in stop_word:\n",
    "            list_query.append(w)\n",
    "\n",
    "    vsm_=np.zeros((len(vsm_vector),len(vsm_vector[0])-1))\n",
    "    query=np.zeros((len(vsm_vector),1))\n",
    "\n",
    "    for i in range(len(vsm_)): #vsm\n",
    "        for j in range(len(vsm_[i])):\n",
    "            vsm_[i][j]=vsm_vector[i][j+1]\n",
    "\n",
    "        for a in range(len(list_query)):\n",
    "            if vsm_vector[i][0] == list_query[a]:\n",
    "                query[i][0] = query[i][0] + 1\n",
    "                \n",
    "    print(\"Vector space model(word by num_doc) :\")\n",
    "    print(vsm_)\n",
    "    print(\"Query(word by num_doc) :\")\n",
    "    print(query)\n",
    "    \n",
    "    sim=np.zeros(len(vsm_[0]))\n",
    "    \n",
    "#     vsm_t=vsm_.T\n",
    "#     query_t=query.T\n",
    "    \n",
    "    \n",
    "#     for i in range(len(vsm_t)):\n",
    "#         sim[i]=cos_sim(vsm_t[i],query_t[0])\n",
    "        \n",
    "        \n",
    "    U, S, V_T = np.linalg.svd(vsm_, full_matrices=False) #SVD 적용 해서 vsm을 u,s,v_T 로 분해해\n",
    "\n",
    "    print(\"matrix U(num_doc by num_doc) : \")\n",
    "    print(U)\n",
    "    print(\"maxtrix S : \")\n",
    "    print(S)\n",
    "    print(\"matrix V_T(num_doc by num_doc) : \")\n",
    "    print(V_T)\n",
    "\n",
    "    k = 2\n",
    "\n",
    "    U_k=np.zeros((len(U),k))\n",
    "    S_k=np.zeros((k,k))\n",
    "    V_T_k=np.zeros((k,len(vsm_[0])))\n",
    "\n",
    "\n",
    "    for i in range(len(U)):   # k 값에 대해 dimension 축소\n",
    "        for j in range(0,k):\n",
    "            U_k[i][j]=U[i][j]\n",
    "    for i in range(0,k):\n",
    "        S_k[i][i]=S[i]\n",
    "    for i in range(0,k):\n",
    "        for j in range(0,len(vsm_[0])):\n",
    "            V_T_k[i][j]=V_T[i][j]\n",
    "\n",
    "    S_k_inv=np.linalg.inv(S_k)\n",
    "    q=np.dot(query.T,np.dot(U_k,S_k_inv))\n",
    "    \n",
    "    V_k=V_T_k.T\n",
    "\n",
    "    \n",
    "    print(q[0])\n",
    "    print(V_k[0])\n",
    "    \n",
    "    for i in range((len(V_k))): #cos similarity 계산\n",
    "        sim[i]=cos_sim(V_k[i],q[0])\n",
    "    \n",
    "    \n",
    "    result=[]\n",
    "    song_top_num=len(song_list[0])\n",
    "\n",
    "    index_art=0\n",
    "    sum=0\n",
    "    \n",
    "    for i in range(len(sim)):\n",
    "        \n",
    "        if i%song_top_num==0 and i>=1:\n",
    "            result.append([singer_list[index_art],sum/song_top_num])\n",
    "            sum=0\n",
    "            index_art+=1\n",
    "        \n",
    "        sum+=sim[i]\n",
    "\n",
    "    \n",
    "    result.append([singer_list[index_art],sum/song_top_num])\n",
    "    result.sort(key=itemgetter(1),reverse=True)    \n",
    "    \n",
    "\n",
    "    print(\"결과값 :\")\n",
    "    for i in range(len(result)):\n",
    "        print(\"artist : \",result[i][0],\"// similarity : \",result[i][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector space model(word by num_doc) :\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Query(word by num_doc) :\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "matrix U(num_doc by num_doc) : \n",
      "[[-7.03665342e-04 -4.48877848e-04  5.96292718e-04 ...  1.28075409e-02\n",
      "   1.67776987e-02  4.30655221e-03]\n",
      " [-3.33580995e-04  6.44764244e-05  2.03441328e-04 ... -3.33190724e-03\n",
      "  -3.88507095e-04  1.52850319e-02]\n",
      " [-3.17777313e-04 -2.94776931e-04  3.65318737e-04 ...  1.45789960e-02\n",
      "   3.55395216e-02 -4.32629073e-04]\n",
      " ...\n",
      " [-1.55887532e-03  1.28260897e-03  3.57972385e-04 ... -9.06150761e-04\n",
      "   6.94025501e-04 -6.82541145e-05]\n",
      " [-1.17217924e-03 -5.39797755e-04  5.01496487e-04 ...  5.24864579e-04\n",
      "  -8.85326904e-04 -9.46368933e-04]\n",
      " [-1.40482090e-03 -8.31144787e-04  2.67664114e-04 ... -2.35618137e-03\n",
      "  -3.63470899e-04  4.12784947e-04]]\n",
      "maxtrix S : \n",
      "[4.26135456e+02 2.62428782e+02 2.49349706e+02 2.18196854e+02\n",
      " 2.11208173e+02 2.06065165e+02 1.99685928e+02 1.97212520e+02\n",
      " 1.94380274e+02 1.91295097e+02 1.90658755e+02 1.86567456e+02\n",
      " 1.82438976e+02 1.77644384e+02 1.73555822e+02 1.71748968e+02\n",
      " 1.67843947e+02 1.65434885e+02 1.64226690e+02 1.61985067e+02\n",
      " 1.59165341e+02 1.57794919e+02 1.56437143e+02 1.55100338e+02\n",
      " 1.52669370e+02 1.52275299e+02 1.50977354e+02 1.50469248e+02\n",
      " 1.49195441e+02 1.48904807e+02 1.47557687e+02 1.47149406e+02\n",
      " 1.46386396e+02 1.45608638e+02 1.45157720e+02 1.44831464e+02\n",
      " 1.42367756e+02 1.41880635e+02 1.40421089e+02 1.39898931e+02\n",
      " 1.39194120e+02 1.37486596e+02 1.36823728e+02 1.36553381e+02\n",
      " 1.35395908e+02 1.35308982e+02 1.34462344e+02 1.33422462e+02\n",
      " 1.32720919e+02 1.32398501e+02 1.31822151e+02 1.31612063e+02\n",
      " 1.31133113e+02 1.30098787e+02 1.29689123e+02 1.29396224e+02\n",
      " 1.28448031e+02 1.28310699e+02 1.27739780e+02 1.27038219e+02\n",
      " 1.26049516e+02 1.25386305e+02 1.25044909e+02 1.24759938e+02\n",
      " 1.24114386e+02 1.23635759e+02 1.23289640e+02 1.22038757e+02\n",
      " 1.21340303e+02 1.20416663e+02 1.20253968e+02 1.19960191e+02\n",
      " 1.19236197e+02 1.18814413e+02 1.18426817e+02 1.17848523e+02\n",
      " 1.17375825e+02 1.16890155e+02 1.16646170e+02 1.16175270e+02\n",
      " 1.15788905e+02 1.15665006e+02 1.15162858e+02 1.14982488e+02\n",
      " 1.14761194e+02 1.14625845e+02 1.14077175e+02 1.13657015e+02\n",
      " 1.13211780e+02 1.13041894e+02 1.12459394e+02 1.12349574e+02\n",
      " 1.11686973e+02 1.11475663e+02 1.10893736e+02 1.10531547e+02\n",
      " 1.10121554e+02 1.09744027e+02 1.09179463e+02 1.09097815e+02\n",
      " 1.08782405e+02 1.08530202e+02 1.08519678e+02 1.07662230e+02\n",
      " 1.07379504e+02 1.06655745e+02 1.06598956e+02 1.06350062e+02\n",
      " 1.05841293e+02 1.05735807e+02 1.05247274e+02 1.04783556e+02\n",
      " 1.04221983e+02 1.04186141e+02 1.03706895e+02 1.03581348e+02\n",
      " 1.03410498e+02 1.03170874e+02 1.02951805e+02 1.02610344e+02\n",
      " 1.02137876e+02 1.01971573e+02 1.01691743e+02 1.01501916e+02\n",
      " 1.01127554e+02 1.00774070e+02 1.00468685e+02 1.00285347e+02\n",
      " 9.97076405e+01 9.95090429e+01 9.93527443e+01 9.91291692e+01\n",
      " 9.89973050e+01 9.85036000e+01 9.84254506e+01 9.77857350e+01\n",
      " 9.76688831e+01 9.75289181e+01 9.72138099e+01 9.69115244e+01\n",
      " 9.64872673e+01 9.63958688e+01 9.57134270e+01 9.56406816e+01\n",
      " 9.53509803e+01 9.51060576e+01 9.48871110e+01 9.47718784e+01\n",
      " 9.45420934e+01 9.39997986e+01 9.38995966e+01 9.35596393e+01\n",
      " 9.32377007e+01 9.28091442e+01 9.25971358e+01 9.21670137e+01\n",
      " 9.19007052e+01 9.15766620e+01 9.13025140e+01 9.10822448e+01\n",
      " 9.07292539e+01 9.06113005e+01 9.03076056e+01 8.98758573e+01\n",
      " 8.94998093e+01 8.94195448e+01 8.90667556e+01 8.84594304e+01\n",
      " 8.83766485e+01 8.80516090e+01 8.78086355e+01 8.73542747e+01\n",
      " 8.71440323e+01 8.69716259e+01 8.67809171e+01 8.66183410e+01\n",
      " 8.61720263e+01 8.60574380e+01 8.55538483e+01 8.52349610e+01\n",
      " 8.50446959e+01 8.48937286e+01 8.45872281e+01 8.42659189e+01\n",
      " 8.39206828e+01 8.37489383e+01 8.34734426e+01 8.33378957e+01\n",
      " 8.28545068e+01 8.27064691e+01 8.25188658e+01 8.21891935e+01\n",
      " 8.18830138e+01 8.13001524e+01 8.07008211e+01 8.04703064e+01\n",
      " 8.01325636e+01 7.98030153e+01 7.93467215e+01 7.91806424e+01\n",
      " 7.88629873e+01 7.83021324e+01 7.80948338e+01 7.79056835e+01\n",
      " 7.74983487e+01 7.72165391e+01 7.66879224e+01 7.64421474e+01\n",
      " 7.63447924e+01 7.62132127e+01 7.58748261e+01 7.55113265e+01\n",
      " 7.53350140e+01 7.50426692e+01 7.48236030e+01 7.44194841e+01\n",
      " 7.40640495e+01 7.40129835e+01 7.38804340e+01 7.34431588e+01\n",
      " 7.33788496e+01 7.30143198e+01 7.28934369e+01 7.26046988e+01\n",
      " 7.23908426e+01 7.22928603e+01 7.20328731e+01 7.18218781e+01\n",
      " 7.13266018e+01 7.10504024e+01 7.08203172e+01 7.07143832e+01\n",
      " 7.03031486e+01 6.98952816e+01 6.97220020e+01 6.94126953e+01\n",
      " 6.92915140e+01 6.91228927e+01 6.87495682e+01 6.85279020e+01\n",
      " 6.83736288e+01 6.82875594e+01 6.80553880e+01 6.77223795e+01\n",
      " 6.75009491e+01 6.71813982e+01 6.70049547e+01 6.68179176e+01\n",
      " 6.66973114e+01 6.65576322e+01 6.62148603e+01 6.61183471e+01\n",
      " 6.59420400e+01 6.57360549e+01 6.56019506e+01 6.53238187e+01\n",
      " 6.52730516e+01 6.50831685e+01 6.48861332e+01 6.47915230e+01\n",
      " 6.45962690e+01 6.43848948e+01 6.42373708e+01 6.38977578e+01\n",
      " 6.38224119e+01 6.37534805e+01 6.35542382e+01 6.34148923e+01\n",
      " 6.31447283e+01 6.27235785e+01 6.24581666e+01 6.22451851e+01\n",
      " 6.21538469e+01 6.18995485e+01 6.17762690e+01 6.15648331e+01\n",
      " 6.12959886e+01 6.12681699e+01 6.10651896e+01 6.08961270e+01\n",
      " 6.07521569e+01 6.06500689e+01 6.03020591e+01 6.01046745e+01\n",
      " 6.00230531e+01 5.99217501e+01 5.95582223e+01 5.92244590e+01\n",
      " 5.91691086e+01 5.90238125e+01 5.88078990e+01 5.87395901e+01\n",
      " 5.86668456e+01 5.84885691e+01 5.84052454e+01 5.82135257e+01\n",
      " 5.78149870e+01 5.76200530e+01 5.74241066e+01 5.69678190e+01\n",
      " 5.67184368e+01 5.65717316e+01 5.62626680e+01 5.61255260e+01\n",
      " 5.58683989e+01 5.56675139e+01 5.53635802e+01 5.51578757e+01\n",
      " 5.51303679e+01 5.50616317e+01 5.48553627e+01 5.45045787e+01\n",
      " 5.43909280e+01 5.41219366e+01 5.40226403e+01 5.39354817e+01\n",
      " 5.36665999e+01 5.35940908e+01 5.34699271e+01 5.32990954e+01\n",
      " 5.31092603e+01 5.26508010e+01 5.25310331e+01 5.22893972e+01\n",
      " 5.21056086e+01 5.18531578e+01 5.15446222e+01 5.14452585e+01\n",
      " 5.09978772e+01 5.09056302e+01 5.08352908e+01 5.07334259e+01\n",
      " 5.01084438e+01 4.99698213e+01 4.98838372e+01 4.96758431e+01\n",
      " 4.92265241e+01 4.89068918e+01 4.87912726e+01 4.83932364e+01\n",
      " 4.82804798e+01 4.80343571e+01 4.78216093e+01 4.77363107e+01\n",
      " 4.73885441e+01 4.71257166e+01 4.67566464e+01 4.61303111e+01\n",
      " 4.59968031e+01 4.58403241e+01 4.50349781e+01 4.42700466e+01\n",
      " 4.42024800e+01 4.38251958e+01 4.32025706e+01 4.29673906e+01\n",
      " 4.27830374e+01 4.25783422e+01 4.24268674e+01 4.19978415e+01\n",
      " 4.18689033e+01 4.16702624e+01 4.13509765e+01 4.11487172e+01\n",
      " 4.08516062e+01 4.03478457e+01 4.02438347e+01 3.93862277e+01\n",
      " 3.89953467e+01 3.89108553e+01 3.80730019e+01 3.72989950e+01\n",
      " 3.67171451e+01 3.56693370e+01 3.54403018e+01 3.51753086e+01\n",
      " 3.50408412e+01 3.41191593e+01 3.39999159e+01 3.28190918e+01\n",
      " 3.18728393e+01 2.88202382e+01 2.62789344e+01 2.52523869e+01\n",
      " 2.26639786e+01 1.62116183e+01 1.26070538e+01 1.20039627e+01\n",
      " 9.95507415e+00 9.38557925e+00 7.85193679e+00 7.62640977e+00\n",
      " 7.03260176e+00 3.08801702e+00 1.91517353e-12 9.15883096e-13\n",
      " 7.47453859e-13 5.77119441e-13 5.37336217e-13 5.22890731e-13\n",
      " 4.78824754e-13 4.74260080e-13 4.19296681e-13 3.91228240e-13\n",
      " 3.68739094e-13 3.60299647e-13 3.59165694e-13 3.49223327e-13\n",
      " 3.48696560e-13 3.31898026e-13 3.31312241e-13 3.12606700e-13\n",
      " 2.59969088e-13 2.38011413e-13 2.07316581e-13 1.92124694e-13\n",
      " 1.73239246e-13 1.54591442e-13 1.38914629e-13 1.25112011e-13\n",
      " 1.20794333e-13 1.13313959e-13 1.02199407e-13 9.00938855e-14\n",
      " 8.58570999e-14 7.02113389e-14 6.86137993e-14 6.13836961e-14\n",
      " 5.82414456e-14 5.13100653e-14 5.06479795e-14 4.03589890e-14\n",
      " 3.86103498e-14 3.28378651e-14 3.16815829e-14 2.97602504e-14\n",
      " 2.75918804e-14 2.75918804e-14 2.17902321e-14 1.49995270e-14\n",
      " 6.35761194e-15]\n",
      "matrix V_T(num_doc by num_doc) : \n",
      "[[-1.48087026e-01 -7.21258727e-02 -1.04503101e-01 ... -4.78967703e-02\n",
      "  -3.50411430e-02 -2.15502078e-02]\n",
      " [-5.54357208e-02 -2.98763808e-02 -5.15939906e-02 ... -9.90793991e-03\n",
      "  -3.90787659e-03 -1.14478059e-02]\n",
      " [ 1.12940179e-01  9.53023672e-03  5.31536011e-02 ...  1.96712471e-02\n",
      "   2.11945959e-02  6.02864644e-04]\n",
      " ...\n",
      " [ 0.00000000e+00  3.20923843e-17  1.58646762e-02 ...  4.74880552e-17\n",
      "   7.89438732e-04  8.79287962e-17]\n",
      " [ 0.00000000e+00 -1.63626437e-16 -3.03849379e-02 ... -1.51571464e-16\n",
      "  -9.46849150e-04  2.01661604e-17]\n",
      " [ 0.00000000e+00  1.60461922e-16  2.37507770e-02 ... -2.35813973e-17\n",
      "   9.73713544e-05 -9.93129190e-17]]\n",
      "[-0.03231637 -0.00787544]\n",
      "[-0.14808703 -0.05543572]\n",
      "결과값 :\n",
      "artist :  ghostface-killah // similarity :  0.9967578534206257\n",
      "artist :  chris-brown // similarity :  0.9955452054647553\n",
      "artist :  ciara // similarity :  0.9954947513547736\n",
      "artist :  billy-ray-cyrus // similarity :  0.9945336048738331\n",
      "artist :  bryan-adams // similarity :  0.9931264944755659\n",
      "artist :  beyonce-knowles // similarity :  0.9912223240369031\n",
      "artist :  the-doors // similarity :  0.9889413067112228\n",
      "artist :  the-calling // similarity :  0.9883237821112313\n",
      "artist :  beatles // similarity :  0.9881817312361776\n",
      "artist :  ariana-grande // similarity :  0.9870566281756578\n",
      "artist :  glen-campbell // similarity :  0.986711993679736\n",
      "artist :  ed-sheeran // similarity :  0.9851504244270841\n",
      "artist :  dr-dre // similarity :  0.9825258518887396\n",
      "artist :  brad-paisley // similarity :  0.9815918183808963\n",
      "artist :  eminem // similarity :  0.9807243871291308\n",
      "artist :  50-cent // similarity :  0.9793140013180475\n",
      "artist :  garth-brooks // similarity :  0.9779163055089322\n",
      "artist :  coldplay // similarity :  0.9775104321742056\n",
      "artist :  eddy-arnold // similarity :  0.9703908705984371\n",
      "artist :  drake // similarity :  0.9178203772656113\n",
      "artist :  2pac // similarity :  0.6219008351795077\n"
     ]
    }
   ],
   "source": [
    "sample_data=pd.read_csv(files)\n",
    "\n",
    "lsa_model(sample_data.iloc[0]['lyrics'],vsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
